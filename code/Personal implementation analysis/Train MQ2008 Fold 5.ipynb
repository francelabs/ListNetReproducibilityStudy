{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on LETOR 4- FOLD 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this version, the gridsearch is limited to those hyper parameters were found to work best. Another combination might be found by personalizing the ranges of the values of interest (learning rate, epochs, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that changes form one training file to another is the path. Based on this, the train validation and test set will be different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the enverionment seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "path='../../MQ2008/Fold5/'\n",
    "from utils import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging\n",
    "my_seed=1\n",
    "set_libraries_seeds(my_seed) #Important to approach reproductible results \n",
    "#tf.logging.set_verbosity(tf.logging.ERROR) #setting a lower verbosity\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vali=load_data(path+'vali.txt')\n",
    "data_train=load_data(path+'train.txt')\n",
    "data_test=load_data(path+'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that the needed folders to perform evaluations, are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory ../../MQ2008/Fold5/models  already exists\n",
      "The directory ../../MQ2008/Fold5/new_predictions  already exists\n",
      "The directory ../../MQ2008/Fold5/performance  already exists\n"
     ]
    }
   ],
   "source": [
    "create_folders(path,[\"models\",\"new_predictions\",\"performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "from keras.regularizers import l1\n",
    "t_ini=time.time()\n",
    "from itertools import product,count\n",
    "#epochs=[500,1000]\n",
    "#learning_rate=[0.001,0.075]\n",
    "#mon=[0.3,0.5]\n",
    "#act=['sigmoid','linear']\n",
    "epochs=[1000]\n",
    "learning_rate=[0.001]\n",
    "mon=[0.3]\n",
    "act=['linear']\n",
    "initializer=keras.initializers.glorot_uniform(seed=my_seed)\n",
    "bias=0.1\n",
    "total_it=len(epochs)*len(learning_rate)*len(mon)*len(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=False #if we wanted to save the models, we would simply have to set this as true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linear_model_iteration(my_iter,name,counter):\n",
    "    \"\"\"\n",
    "    Create a single linear model\n",
    "    \n",
    "    my_iter: array of 4 hyper-parameters which must respectively be:\n",
    "        - 0 => number of iteration\n",
    "        - 1 => learning rate\n",
    "        - 2 => momentum\n",
    "        - 3 => activation function\n",
    "        \n",
    "    name: the name given to the model file\n",
    "    \n",
    "    counter: the number count of the model, will be added as a suffix to the name\n",
    "    \"\"\"\n",
    "    print(\"Iteration:\",counter,\"/\",total_it)\n",
    "    name=name+str(counter)\n",
    "    \n",
    "    x_list_train,y_list_train=get_list_xy(data_train)\n",
    "    \n",
    "    n_iter=my_iter[0]\n",
    "    n_=my_iter[1]\n",
    "    mom_=my_iter[2]\n",
    "    act_=my_iter[3]\n",
    "\n",
    "    modelq = Sequential()\n",
    "    modelq.add(Dense(1,input_dim=x_list_train[0].shape[1], activation=act_,\n",
    "                     kernel_initializer=initializer, bias_initializer=Constant(value=bias)))\n",
    "    opt = SGD(lr=n_, momentum=mom_)\n",
    "\n",
    "    modelq.compile(loss=Loss_query_keras, optimizer=opt)\n",
    "    #display(modelq.summary())\n",
    "    \n",
    "\n",
    "    for j in tqdm(range(n_iter)):\n",
    "\n",
    "        for ki in range(len(y_list_train)):\n",
    "            if act_=='sigmoid':\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki]/2)\n",
    "            else:\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki])\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    print(\"Time elapsed so far:\")\n",
    "    print(convert_to_time(time.time()-t_ini))\n",
    "    print(my_iter,\"\\n\\n\\n\")\n",
    "    if(save_model):\n",
    "        modelq.save(path+'new_models/model_'+name+'.h5')\n",
    "    \n",
    "\n",
    "    y_pred_train=modelq.predict(data_train.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_train=pd.DataFrame(y_pred_train)\n",
    "    df_train.to_csv(path+'new_predictions/y_train_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "    y_pred_vali=modelq.predict(data_vali.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_vali=pd.DataFrame(y_pred_vali)\n",
    "    df_vali.to_csv(path+'new_predictions/y_vali_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "\n",
    "    y_pred_test=modelq.predict(data_test.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_test=pd.DataFrame(y_pred_test)\n",
    "    df_test.to_csv(path+'new_predictions/y_test_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "    print(\"                             \",counter,\"                                     \")\n",
    "    print(\"=================================================================\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs=4,verbose=50)(delayed(linear_model_iteration)(my_iter,'LETOR',counter) \n",
    "                               for(my_iter,counter) in zip(product(epochs,learning_rate,mon,act),\n",
    "                                                                   range(1,total_it+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a relu activation functions as it converges faster and appears to perform better (than sigmoid for instance) according to Literature. \n",
    "\n",
    "The regularization we use is Dropout, which implies randomly shutting down Neurons in the Network, to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "from keras.regularizers import l1\n",
    "t_ini=time.time()\n",
    "from itertools import product\n",
    "#epochs=[300,400,500,600]\n",
    "#learning_rate=[0.0025]\n",
    "#mon=[0.5]\n",
    "#act=['linear']\n",
    "#inner_act=['relu']\n",
    "#number_layers=[1,2]\n",
    "#number_neurons=[100]\n",
    "#dropout=[0.65,0.8]\n",
    "epochs=[600]\n",
    "learning_rate=[0.0025]\n",
    "mon=[0.5]\n",
    "act=['linear']\n",
    "inner_act=['relu']\n",
    "number_layers=[1]\n",
    "number_neurons=[100]\n",
    "dropout=[0.65]\n",
    "counter=1\n",
    "initializer=keras.initializers.glorot_uniform(seed=my_seed)\n",
    "bias=0.1\n",
    "total_it=len(epochs)*len(learning_rate)*len(mon)*len(act)*len(inner_act)\\\n",
    "*len(number_layers)*len(number_neurons)*len(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_iteration(my_iter,name,counter):\n",
    "    \"\"\"\n",
    "    Create a single neural network model\n",
    "    \n",
    "    my_iter: array of 8 hyper-parameters which must respectively be:\n",
    "        - 0 => number of iteration\n",
    "        - 1 => learning rate\n",
    "        - 2 => momentum\n",
    "        - 3 => activation function\n",
    "        - 4 => inner layers activation function\n",
    "        - 5 => number of hidden layers\n",
    "        - 6 => number of neurons by hidden layers\n",
    "        - 7 => dropout\n",
    "        \n",
    "    name: the name given to the model file\n",
    "    \n",
    "    counter: the number count of the model, will be added as a suffix to the name\n",
    "    \"\"\"\n",
    "    name=name+str(counter)\n",
    "    print(\"Iteration:\",counter,\"/\",total_it)\n",
    "\n",
    "    x_list_train,y_list_train=get_list_xy(data_train)\n",
    "    \n",
    "    n_iter=my_iter[0]\n",
    "    n_=my_iter[1]\n",
    "    mom_=my_iter[2]\n",
    "    act_=my_iter[3]\n",
    "    \n",
    "    inner_act_=my_iter[4]\n",
    "    number_layers_=my_iter[5]\n",
    "    number_neurons_=my_iter[6]\n",
    "    dropout_=my_iter[7]\n",
    "\n",
    "    modelq = Sequential()\n",
    "    modelq.add(Dense(number_neurons_,input_dim=x_list_train[0].shape[1], activation=inner_act_,\n",
    "                     kernel_initializer=initializer, bias_initializer=Constant(value=bias)))\n",
    "    modelq.add(Dropout(dropout_))\n",
    "    # Adding hidden layers based on the gridsearch value\n",
    "    \n",
    "    for i in range(number_layers_):\n",
    "        modelq.add(Dense(number_neurons_,activation=inner_act_,\n",
    "                        kernel_initializer=initializer,\n",
    "                        bias_initializer=Constant(value=bias)))\n",
    "        modelq.add(Dropout(dropout_))\n",
    "\n",
    "    modelq.add(Dense(1,activation=act_,\n",
    "                kernel_initializer=initializer,\n",
    "                bias_initializer=Constant(value=bias)))\n",
    "    opt = SGD(lr=n_, momentum=mom_)\n",
    "\n",
    "    modelq.compile(loss=Loss_query_keras, optimizer=opt)\n",
    "    #display(modelq.summary())\n",
    "    \n",
    "\n",
    "    for j in tqdm(range(n_iter)):\n",
    "\n",
    "        for ki in range(len(y_list_train)):\n",
    "            if act_=='sigmoid':\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki]/2)\n",
    "            else:\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki])\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    print(\"Time elapsed so far:\")\n",
    "    print(convert_to_time(time.time()-t_ini))\n",
    "    print(my_iter,\"\\n\\n\\n\")\n",
    "    if(save_model):\n",
    "        modelq.save(path+'models/model_'+name+'.h5')\n",
    "    \n",
    "\n",
    "    y_pred_train=modelq.predict(data_train.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_train=pd.DataFrame(y_pred_train)\n",
    "    df_train.to_csv(path+'new_predictions/y_train_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "    y_pred_vali=modelq.predict(data_vali.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_vali=pd.DataFrame(y_pred_vali)\n",
    "    df_vali.to_csv(path+'new_predictions/y_vali_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "\n",
    "    y_pred_test=modelq.predict(data_test.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_test=pd.DataFrame(y_pred_test)\n",
    "    df_test.to_csv(path+'new_predictions/y_test_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "    print(\"                             \",counter,\"                                     \")\n",
    "    print(\"=================================================================\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs=4,verbose=50)(delayed(NN_model_iteration)(my_iter,'LETOR_NN',counter) \n",
    "                               for(my_iter,counter) in zip(product(epochs,learning_rate,mon,act,\n",
    "                                                                   inner_act,number_layers,number_neurons,dropout),\n",
    "                                                           range(1,total_it+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
