{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on LETOR 4- FOLD 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this version, the gridsearch is limited to those hyper parameters were found to work best. Another combination might be found by personalizing the ranges of the values of interest (learning rate, epochs, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that changes form one training file to another is the path. Based on this, the train validation and test set will be different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the enverionment seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "path='../../MQ2007/Fold5/'\n",
    "from utils import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging\n",
    "my_seed=1\n",
    "set_libraries_seeds(my_seed) #Important to approach reproductible results \n",
    "#tf.logging.set_verbosity(tf.logging.ERROR) #setting a lower verbosity\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vali=load_data(path+'vali.txt')\n",
    "data_train=load_data(path+'train.txt')\n",
    "data_test=load_data(path+'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that the needed folders to perform evaluations, are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory ../../MQ2007/Fold5/models  already exists\n",
      "The directory ../../MQ2007/Fold5/new_predictions  already exists\n",
      "The directory ../../MQ2007/Fold5/performance  already exists\n"
     ]
    }
   ],
   "source": [
    "create_folders(path,[\"models\",\"new_predictions\",\"performance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "from keras.regularizers import l1\n",
    "t_ini=time.time()\n",
    "from itertools import product,count\n",
    "#epochs=[500,1000]\n",
    "#learning_rate=[0.001,0.075]\n",
    "#mon=[0.3,0.5]\n",
    "#act=['sigmoid','linear']\n",
    "epochs=[1000]\n",
    "learning_rate=[0.001]\n",
    "mon=[0.5]\n",
    "act=['linear']\n",
    "initializer=keras.initializers.glorot_uniform(seed=my_seed)\n",
    "bias=0.1\n",
    "total_it=len(epochs)*len(learning_rate)*len(mon)*len(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=False #if we wanted to save the models, we would simply have to set this as true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linear_model_iteration(my_iter,name,counter):\n",
    "    \"\"\"\n",
    "    Create a single linear model\n",
    "    \n",
    "    my_iter: array of 4 hyper-parameters which must respectively be:\n",
    "        - 0 => number of iteration\n",
    "        - 1 => learning rate\n",
    "        - 2 => momentum\n",
    "        - 3 => activation function\n",
    "        \n",
    "    name: the name given to the model file\n",
    "    \n",
    "    counter: the number count of the model, will be added as a suffix to the name\n",
    "    \"\"\"\n",
    "    print(\"Iteration:\",counter,\"/\",total_it)\n",
    "    name=name+str(counter)\n",
    "    \n",
    "    x_list_train,y_list_train=get_list_xy(data_train)\n",
    "    \n",
    "    n_iter=my_iter[0]\n",
    "    n_=my_iter[1]\n",
    "    mom_=my_iter[2]\n",
    "    act_=my_iter[3]\n",
    "\n",
    "    modelq = Sequential()\n",
    "    modelq.add(Dense(1,input_dim=x_list_train[0].shape[1], activation=act_,\n",
    "                     kernel_initializer=initializer, bias_initializer=Constant(value=bias)))\n",
    "    opt = SGD(lr=n_, momentum=mom_)\n",
    "\n",
    "    modelq.compile(loss=Loss_query_keras, optimizer=opt)\n",
    "    #display(modelq.summary())\n",
    "    \n",
    "\n",
    "    for j in tqdm(range(n_iter)):\n",
    "\n",
    "        for ki in range(len(y_list_train)):\n",
    "            if act_=='sigmoid':\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki]/2)\n",
    "            else:\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki])\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    print(\"Time elapsed so far:\")\n",
    "    print(convert_to_time(time.time()-t_ini))\n",
    "    print(my_iter,\"\\n\\n\\n\")\n",
    "    if(save_model):\n",
    "        modelq.save(path+'new_models/model_'+name+'.h5')\n",
    "    \n",
    "\n",
    "    y_pred_train=modelq.predict(data_train.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_train=pd.DataFrame(y_pred_train)\n",
    "    df_train.to_csv(path+'new_predictions/y_train_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "    y_pred_vali=modelq.predict(data_vali.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_vali=pd.DataFrame(y_pred_vali)\n",
    "    df_vali.to_csv(path+'new_predictions/y_vali_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "\n",
    "    y_pred_test=modelq.predict(data_test.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_test=pd.DataFrame(y_pred_test)\n",
    "    df_test.to_csv(path+'new_predictions/y_test_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "    print(\"                             \",counter,\"                                     \")\n",
    "    print(\"=================================================================\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=2)]: Done   2 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=2)]: Done   3 tasks      | elapsed: 23.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keustor/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=2)]: Done   5 tasks      | elapsed: 33.2min\n",
      "[Parallel(n_jobs=2)]: Done   6 tasks      | elapsed: 36.5min\n",
      "[Parallel(n_jobs=2)]: Done   7 tasks      | elapsed: 46.4min\n",
      "[Parallel(n_jobs=2)]: Done   8 tasks      | elapsed: 46.7min\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed: 66.6min\n",
      "[Parallel(n_jobs=2)]: Done  10 tasks      | elapsed: 72.8min\n",
      "[Parallel(n_jobs=2)]: Done  11 tasks      | elapsed: 93.9min\n",
      "[Parallel(n_jobs=2)]: Done  12 tasks      | elapsed: 94.6min\n",
      "[Parallel(n_jobs=2)]: Done  13 tasks      | elapsed: 111.7min\n",
      "[Parallel(n_jobs=2)]: Done  14 out of  16 | elapsed: 117.0min remaining: 16.7min\n",
      "[Parallel(n_jobs=2)]: Done  16 out of  16 | elapsed: 134.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done  16 out of  16 | elapsed: 134.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs=2,verbose=50)(delayed(linear_model_iteration)(my_iter,'LETOR',counter) \n",
    "                               for(my_iter,counter) in zip(product(epochs,learning_rate,mon,act),\n",
    "                                                                   range(1,total_it+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a relu activation functions as it converges faster and appears to perform better (than sigmoid for instance) according to Literature. \n",
    "\n",
    "The regularization we use is Dropout, which implies randomly shutting down Neurons in the Network, to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "from keras.regularizers import l1\n",
    "t_ini=time.time()\n",
    "from itertools import product\n",
    "#epochs=[300,400,500,600]\n",
    "#learning_rate=[0.0025]\n",
    "#mon=[0.5]\n",
    "#act=['linear']\n",
    "#inner_act=['relu']\n",
    "#number_layers=[1,2]\n",
    "#number_neurons=[100]\n",
    "#dropout=[0.65,0.8]\n",
    "epochs=[400]\n",
    "learning_rate=[0.0025]\n",
    "mon=[0.5]\n",
    "act=['linear']\n",
    "inner_act=['relu']\n",
    "number_layers=[2]\n",
    "number_neurons=[100]\n",
    "dropout=[0.65]\n",
    "counter=1\n",
    "initializer=keras.initializers.glorot_uniform(seed=my_seed)\n",
    "bias=0.1\n",
    "total_it=len(epochs)*len(learning_rate)*len(mon)*len(act)*len(inner_act)\\\n",
    "*len(number_layers)*len(number_neurons)*len(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_iteration(my_iter,name,counter):\n",
    "    \"\"\"\n",
    "    Create a single neural network model\n",
    "    \n",
    "    my_iter: array of 8 hyper-parameters which must respectively be:\n",
    "        - 0 => number of iteration\n",
    "        - 1 => learning rate\n",
    "        - 2 => momentum\n",
    "        - 3 => activation function\n",
    "        - 4 => inner layers activation function\n",
    "        - 5 => number of hidden layers\n",
    "        - 6 => number of neurons by hidden layers\n",
    "        - 7 => dropout\n",
    "        \n",
    "    name: the name given to the model file\n",
    "    \n",
    "    counter: the number count of the model, will be added as a suffix to the name\n",
    "    \"\"\"\n",
    "    name=name+str(counter)\n",
    "    print(\"Iteration:\",counter,\"/\",total_it)\n",
    "\n",
    "    x_list_train,y_list_train=get_list_xy(data_train)\n",
    "    \n",
    "    n_iter=my_iter[0]\n",
    "    n_=my_iter[1]\n",
    "    mom_=my_iter[2]\n",
    "    act_=my_iter[3]\n",
    "    \n",
    "    inner_act_=my_iter[4]\n",
    "    number_layers_=my_iter[5]\n",
    "    number_neurons_=my_iter[6]\n",
    "    dropout_=my_iter[7]\n",
    "\n",
    "    modelq = Sequential()\n",
    "    modelq.add(Dense(number_neurons_,input_dim=x_list_train[0].shape[1], activation=inner_act_,\n",
    "                     kernel_initializer=initializer, bias_initializer=Constant(value=bias)))\n",
    "    modelq.add(Dropout(dropout_))\n",
    "    # Adding hidden layers based on the gridsearch value\n",
    "    \n",
    "    for i in range(number_layers_):\n",
    "        modelq.add(Dense(number_neurons_,activation=inner_act_,\n",
    "                        kernel_initializer=initializer,\n",
    "                        bias_initializer=Constant(value=bias)))\n",
    "        modelq.add(Dropout(dropout_))\n",
    "\n",
    "    modelq.add(Dense(1,activation=act_,\n",
    "                kernel_initializer=initializer,\n",
    "                bias_initializer=Constant(value=bias)))\n",
    "    opt = SGD(lr=n_, momentum=mom_)\n",
    "\n",
    "    modelq.compile(loss=Loss_query_keras, optimizer=opt)\n",
    "    #display(modelq.summary())\n",
    "    \n",
    "\n",
    "    for j in tqdm(range(n_iter)):\n",
    "\n",
    "        for ki in range(len(y_list_train)):\n",
    "            if act_=='sigmoid':\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki]/2)\n",
    "            else:\n",
    "                loss_qi=modelq.train_on_batch(x=x_list_train[ki],y=y_list_train[ki])\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    print(\"Time elapsed so far:\")\n",
    "    print(convert_to_time(time.time()-t_ini))\n",
    "    print(my_iter,\"\\n\\n\\n\")\n",
    "    if(save_model):\n",
    "        modelq.save(path+'models/model_'+name+'.h5')\n",
    "    \n",
    "\n",
    "    y_pred_train=modelq.predict(data_train.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_train=pd.DataFrame(y_pred_train)\n",
    "    df_train.to_csv(path+'new_predictions/y_train_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "    y_pred_vali=modelq.predict(data_vali.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_vali=pd.DataFrame(y_pred_vali)\n",
    "    df_vali.to_csv(path+'new_predictions/y_vali_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "\n",
    "\n",
    "    y_pred_test=modelq.predict(data_test.drop(['relevance degree','qid'],axis=1)).astype('float64').ravel()\n",
    "    df_test=pd.DataFrame(y_pred_test)\n",
    "    df_test.to_csv(path+'new_predictions/y_test_'+name+'.txt',sep=' ',header=False,index=False)\n",
    "    print(\"                             \",counter,\"                                     \")\n",
    "    print(\"=================================================================\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=2)]: Done   2 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=2)]: Done   3 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=2)]: Done   5 tasks      | elapsed: 45.7min\n",
      "[Parallel(n_jobs=2)]: Done   6 tasks      | elapsed: 45.9min\n",
      "[Parallel(n_jobs=2)]: Done   7 tasks      | elapsed: 67.1min\n",
      "[Parallel(n_jobs=2)]: Done   8 tasks      | elapsed: 67.3min\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed: 88.2min\n",
      "[Parallel(n_jobs=2)]: Done  10 tasks      | elapsed: 88.3min\n",
      "[Parallel(n_jobs=2)]: Done  11 tasks      | elapsed: 114.9min\n",
      "[Parallel(n_jobs=2)]: Done  12 tasks      | elapsed: 114.9min\n",
      "[Parallel(n_jobs=2)]: Done  13 tasks      | elapsed: 140.4min\n",
      "[Parallel(n_jobs=2)]: Done  14 out of  16 | elapsed: 140.4min remaining: 20.1min\n",
      "[Parallel(n_jobs=2)]: Done  16 out of  16 | elapsed: 171.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done  16 out of  16 | elapsed: 171.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs=2,verbose=50)(delayed(NN_model_iteration)(my_iter,'LETOR_NN',counter) \n",
    "                               for(my_iter,counter) in zip(product(epochs,learning_rate,mon,act,\n",
    "                                                                   inner_act,number_layers,number_neurons,dropout),\n",
    "                                                           range(1,total_it+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
